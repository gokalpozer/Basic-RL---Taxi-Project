{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8830962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2bbf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an environment\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a487df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ee00a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q Table\n",
    "\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7dedcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange Hyperparameters(learning rate, discount rate, epsilon rate for exploitation-exploration)\n",
    "\n",
    "alpha = 0.1 #Learning Rate\n",
    "gamma = 0.9 #Discount Rate \n",
    "epsilon = 0.1 #Epsilon Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b2fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Metrics\n",
    "\n",
    "reward_list = []\n",
    "dropout_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93cf3b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, Reward: -2078, Dropouts: 87\n",
      "Episode: 20, Reward: -1823, Dropouts: 85\n",
      "Episode: 30, Reward: -488, Dropouts: 18\n",
      "Episode: 40, Reward: -357, Dropouts: 10\n",
      "Episode: 50, Reward: -353, Dropouts: 6\n",
      "Episode: 60, Reward: -187, Dropouts: 7\n",
      "Episode: 70, Reward: -234, Dropouts: 6\n",
      "Episode: 80, Reward: -300, Dropouts: 8\n",
      "Episode: 90, Reward: -619, Dropouts: 31\n",
      "Episode: 100, Reward: -189, Dropouts: 7\n",
      "Episode: 110, Reward: -332, Dropouts: 11\n",
      "Episode: 120, Reward: -255, Dropouts: 9\n",
      "Episode: 130, Reward: -141, Dropouts: 4\n",
      "Episode: 140, Reward: -505, Dropouts: 19\n",
      "Episode: 150, Reward: -119, Dropouts: 1\n",
      "Episode: 160, Reward: -100, Dropouts: 3\n",
      "Episode: 170, Reward: -238, Dropouts: 10\n",
      "Episode: 180, Reward: -217, Dropouts: 7\n",
      "Episode: 190, Reward: -233, Dropouts: 10\n",
      "Episode: 200, Reward: -59, Dropouts: 1\n",
      "Episode: 210, Reward: -295, Dropouts: 13\n",
      "Episode: 220, Reward: 10, Dropouts: 0\n",
      "Episode: 230, Reward: -194, Dropouts: 5\n",
      "Episode: 240, Reward: -102, Dropouts: 3\n",
      "Episode: 250, Reward: -364, Dropouts: 16\n",
      "Episode: 260, Reward: -121, Dropouts: 4\n",
      "Episode: 270, Reward: -164, Dropouts: 3\n",
      "Episode: 280, Reward: -109, Dropouts: 3\n",
      "Episode: 290, Reward: -49, Dropouts: 4\n",
      "Episode: 300, Reward: -133, Dropouts: 3\n",
      "Episode: 310, Reward: -20, Dropouts: 2\n",
      "Episode: 320, Reward: -146, Dropouts: 7\n",
      "Episode: 330, Reward: -69, Dropouts: 2\n",
      "Episode: 340, Reward: -26, Dropouts: 2\n",
      "Episode: 350, Reward: -59, Dropouts: 2\n",
      "Episode: 360, Reward: -163, Dropouts: 6\n",
      "Episode: 370, Reward: -104, Dropouts: 4\n",
      "Episode: 380, Reward: -71, Dropouts: 5\n",
      "Episode: 390, Reward: -30, Dropouts: 1\n",
      "Episode: 400, Reward: -106, Dropouts: 6\n",
      "Episode: 410, Reward: -73, Dropouts: 1\n",
      "Episode: 420, Reward: -183, Dropouts: 9\n",
      "Episode: 430, Reward: -135, Dropouts: 4\n",
      "Episode: 440, Reward: -23, Dropouts: 0\n",
      "Episode: 450, Reward: -73, Dropouts: 5\n",
      "Episode: 460, Reward: -25, Dropouts: 0\n",
      "Episode: 470, Reward: -75, Dropouts: 3\n",
      "Episode: 480, Reward: 1, Dropouts: 0\n",
      "Episode: 490, Reward: -40, Dropouts: 1\n",
      "Episode: 500, Reward: 7, Dropouts: 0\n",
      "Episode: 510, Reward: -30, Dropouts: 1\n",
      "Episode: 520, Reward: -54, Dropouts: 2\n",
      "Episode: 530, Reward: -111, Dropouts: 5\n",
      "Episode: 540, Reward: -10, Dropouts: 1\n",
      "Episode: 550, Reward: 1, Dropouts: 1\n",
      "Episode: 560, Reward: -84, Dropouts: 1\n",
      "Episode: 570, Reward: -58, Dropouts: 1\n",
      "Episode: 580, Reward: 5, Dropouts: 0\n",
      "Episode: 590, Reward: 11, Dropouts: 0\n",
      "Episode: 600, Reward: -23, Dropouts: 1\n",
      "Episode: 610, Reward: -65, Dropouts: 2\n",
      "Episode: 620, Reward: -10, Dropouts: 2\n",
      "Episode: 630, Reward: -76, Dropouts: 3\n",
      "Episode: 640, Reward: 1, Dropouts: 0\n",
      "Episode: 650, Reward: 3, Dropouts: 0\n",
      "Episode: 660, Reward: -4, Dropouts: 1\n",
      "Episode: 670, Reward: -15, Dropouts: 1\n",
      "Episode: 680, Reward: -66, Dropouts: 0\n",
      "Episode: 690, Reward: -1, Dropouts: 0\n",
      "Episode: 700, Reward: -48, Dropouts: 1\n",
      "Episode: 710, Reward: -11, Dropouts: 1\n",
      "Episode: 720, Reward: -24, Dropouts: 2\n",
      "Episode: 730, Reward: -6, Dropouts: 0\n",
      "Episode: 740, Reward: -19, Dropouts: 1\n",
      "Episode: 750, Reward: -46, Dropouts: 3\n",
      "Episode: 760, Reward: -33, Dropouts: 2\n",
      "Episode: 770, Reward: -60, Dropouts: 2\n",
      "Episode: 780, Reward: -48, Dropouts: 3\n",
      "Episode: 790, Reward: -81, Dropouts: 3\n",
      "Episode: 800, Reward: 1, Dropouts: 1\n",
      "Episode: 810, Reward: -2, Dropouts: 0\n",
      "Episode: 820, Reward: -20, Dropouts: 1\n",
      "Episode: 830, Reward: 11, Dropouts: 0\n",
      "Episode: 840, Reward: 0, Dropouts: 0\n",
      "Episode: 850, Reward: 11, Dropouts: 0\n",
      "Episode: 860, Reward: -13, Dropouts: 1\n",
      "Episode: 870, Reward: -9, Dropouts: 1\n",
      "Episode: 880, Reward: -16, Dropouts: 0\n",
      "Episode: 890, Reward: 11, Dropouts: 0\n",
      "Episode: 900, Reward: 6, Dropouts: 0\n",
      "Episode: 910, Reward: 8, Dropouts: 0\n",
      "Episode: 920, Reward: -23, Dropouts: 2\n",
      "Episode: 930, Reward: -2, Dropouts: 0\n",
      "Episode: 940, Reward: -2, Dropouts: 1\n",
      "Episode: 950, Reward: -5, Dropouts: 1\n",
      "Episode: 960, Reward: 9, Dropouts: 0\n",
      "Episode: 970, Reward: -122, Dropouts: 5\n",
      "Episode: 980, Reward: 10, Dropouts: 0\n",
      "Episode: 990, Reward: -39, Dropouts: 0\n",
      "Episode: 1000, Reward: 6, Dropouts: 0\n"
     ]
    }
   ],
   "source": [
    "episode_number = 1001\n",
    "\n",
    "for i in range(1,episode_number):\n",
    "    \n",
    "    # Initialize Environment\n",
    "    state = env.reset()\n",
    "    \n",
    "    reward_count = 0\n",
    "    dropouts = 0\n",
    "    \n",
    "    while True:\n",
    "        # Exploit-Explore decision to find an action(%10 explore, %90 exploit)\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])#En yuksek action in bulundugu index i veriyor\n",
    "        \n",
    "        # Take action and observe outcome state(s') and reward.\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Q Learning Function\n",
    "        old_value = q_table[state, action]#old_value\n",
    "        next_max = np.max(q_table[next_state])#next_max\n",
    "        \n",
    "        next_value = (1-alpha) * old_value + alpha * (reward+ gamma*next_max)# Main Function\n",
    "        \n",
    "        # Update Q Table\n",
    "        q_table[state, action] = next_value\n",
    "        \n",
    "        # Update State\n",
    "        state = next_state\n",
    "        \n",
    "        # Find Wrong Dropouts(just for fun) and update total reward\n",
    "        if reward == -10:\n",
    "            dropouts += 1\n",
    "        \n",
    "        reward_count += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        dropout_list.append(dropouts)\n",
    "        reward_list.append(reward_count)\n",
    "        print(\"Episode: {}, Reward: {}, Dropouts: {}\".format(i, reward_count, dropouts))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "814e415e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.78071684, -4.74384053, -4.74453871, -4.79151472, -5.78053726,\n",
       "       -6.2296297 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[246]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc90610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[42mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "while True:\n",
    "    # Exploit-Explore decision to find an action(%10 explore, %90 exploit)\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_table[state])#En yuksek action in bulundugu index i veriyor\n",
    "        \n",
    "    # Take action and observe outcome state(s') and reward.\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "    # Q Learning Function\n",
    "    old_value = q_table[state, action]#old_value\n",
    "    next_max = np.max(q_table[next_state])#next_max\n",
    "        \n",
    "    next_value = (1-alpha) * old_value + alpha * (reward+ gamma*next_max)# Main Function\n",
    "        \n",
    "    # Update Q Table\n",
    "    q_table[state, action] = next_value\n",
    "        \n",
    "    # Update State\n",
    "    state = next_state\n",
    "        \n",
    "    \n",
    "    env.render()\n",
    "        \n",
    "    if done:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
